{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxbb8FLuJUx7"
   },
   "source": [
    "# Линейная регрессия\n",
    "\n",
    "![](https://habrastorage.org/files/256/a5d/ed0/256a5ded03274e0f87ccf97164c31c35.png)\n",
    "\n",
    "## Теория\n",
    "\n",
    "Линейная регрессия - один из наиболее хорошо изученных методов машинного обучения, позволяющий прогнозировать значения количественного признака в виде линейной комбинации прочих признаков с параметрами - весами модели. Оптимальные (в смысле минимальности некоторого функционала ошибки) параметры линейной регрессии можно найти аналитически с помощью нормального уравнения или численно с помощью методов оптимизации.\n",
    "\n",
    "Пусть есть некоторый числовой целевой признак $y$. Извеcтны признаки $X = [x_1, x_2, ... ,x_m]$. Модель регрессии предсказывает $y$ с помощью вычисления следующей взвешенной суммы:\n",
    "\n",
    "$$\n",
    "y = w_0 + \\sum_{i=1}^{m} {w_i x_i} \\tag{1}\n",
    "$$\n",
    "\n",
    "Добавив фиктивный признак $x_0 = 1$, можно будет записать:\n",
    "\n",
    "$y = \\sum_{i=0}^m w_i x_i = \\vec{w}^T \\vec{x} \\tag{2}$\n",
    "\n",
    "Если представить себе $X$ как матрицу объектов-признаков, $y$ - вектор столбец целевой переменной, а $w$ - вектор-столбец коэффициентов, то получим матричную запись:\n",
    "\n",
    "$$\n",
    "\\Large \\vec{y} = X\\vec{w} \\tag{3}\n",
    "$$\n",
    "\n",
    "## MSE — Среднеквадратичный Функция потерь\n",
    "\n",
    "Очевидно, что в уравнениях $(1 - 3)$ нам известны значения $X$ и $y$ — это константы в некотором роде, так как это обучающая выборка. \n",
    "\n",
    "Чтобы подобрать значения коэффициентов (они же веса) $w$, нам необходима функция ошибки (loss-функция):\n",
    "\n",
    "$$\n",
    "MSE\\left(\\overline{y}, y\\right) = \\frac{1}{n} \\sum_{i=0}^{n} {\\left(y_i - \\overline{y_i}\\right)^2}, \\tag{4}\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "MSE(X, w_0, ... , w_1, y) = \\frac{1}{n} \\sum_{i=1}^{n} {\\left(y_i - w_0 + w_1 x_1 + ... + w_n x_n\\right)^2} \\tag{5}\n",
    "$$\n",
    "\n",
    "Функция ошибки позволяет оценить точность модели в числовом виде — чем меньше ее значение, тем выше точность модели.\n",
    "\n",
    "## МНК — Метод Наименьших Квадратов\n",
    "\n",
    "Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью:\n",
    "$$\n",
    "\\large\n",
    "\\begin{array}\n",
    "{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Для решения данной оптимизационной задачи необходимо вычислить производные по параметрам модели, приравнять их к нулю и решить полученные уравнения относительно $\\vec w$ (частный случай был в лекции):\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{array}\n",
    "{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) = 0 \\\\\n",
    "&\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} = 0 \\\\\n",
    "&\\Leftrightarrow& X^T X \\vec{w} = X^T \\vec{y} \\\\\n",
    "&\\Leftrightarrow& \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Нормальное уравнение\n",
    "\n",
    "Таким образом для аналитического вычисления весов линейной регрессии нужно решить это нормальное уравнение: \n",
    "\n",
    "$$\n",
    "\\Large \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y} \\tag{6}\n",
    "$$\n",
    "\n",
    "Матрица ${(X^TX)}^{-1}X^T$ - [псевдообратная](https://ru.wikipedia.org/wiki/%D0%9F%D1%81%D0%B5%D0%B2%D0%B4%D0%BE%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0) для матрицы $X$. В NumPy такую матрицу можно вычислить с помощью функции [numpy.linalg.pinv](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.pinv.html).\n",
    "Однако, нахождение псевдообратной матрицы - операция вычислительно сложная и нестабильная в случае малого определителя матрицы $X$ (проблема [мультиколлинеарности](https://ru.wikipedia.org/wiki/%D0%9C%D1%83%D0%BB%D1%8C%D1%82%D0%B8%D0%BA%D0%BE%D0%BB%D0%BB%D0%B8%D0%BD%D0%B5%D0%B0%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C)). На практике лучше находить вектор весов $w$ решением матричного уравнения $$\\Large X^TXw = X^Ty$$Это может быть сделано с помощью функции [numpy.linalg.solve](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.solve.html).\n",
    "Но все же на практике для больших матриц $X$ быстрее работает градиентный спуск, особенно его стохастическая версия.\n",
    "\n",
    "## Градиентный спуск\n",
    "\n",
    "### Градиент\n",
    "\n",
    "Уравнение линейной регрессии завит от нескольких переменных (если число признаков больше одного), соответсвенно мы имеем дело с функцией нескольких переменных. Градиент такой функции — это вектор частных производных, указывающий направление наискорейшего _роста_ функции:\n",
    "$$\n",
    "\\nabla{y(w_0, w_1, ... , w_n)} = \\left({\\frac{\\partial y}{\\partial w_0}}, {\\frac{\\partial y}{\\partial w_1}}, ... , {\\frac{\\partial y}{\\partial w_n}} \\right)^T \\tag{7}\n",
    "$$\n",
    "\n",
    "Посчитаем градиент для функции потерь MSE $(5)$:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{array}\n",
    "{rcl}\\mathcal{L} &=& MSE(w_0, w_1, ... , w_n),\\\\\n",
    "&=& \\frac{1}{n} \\sum_{i=1}^{n} {\\left(y_i - w_0 + w_1 x_1 + ... + w_n x_n\\right)^2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Частные производные по $w_0$ и $w_i$ отдельно:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial w_0} =& \\frac{2}{n} \\sum_{i=1}^{n} {\\left(w_0 + w_1 x_1 + ... + w_n x_n - y_i\\right)}, \\tag{8} \\\\\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial w_j} =& \\frac{2}{n} \\sum_{i=1}^{n} {x_j \\left(w_0 + w_1 x_1 + ... + w_n x_n - y_i\\right)},\\ j \\in \\{1, \\dots , n\\} \\tag{9}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Итого:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W_j} = \\frac{2}{n} \\sum_{i=1}^{n} {x_i^j \\left(\\overline{y}_i - y_i\\right)} \\tag{10}\n",
    "$$\n",
    "\n",
    "\n",
    "Параметры $w_0, w_1, w_2, w_3$, по которым минимизируется среднеквадратичная ошибка, можно находить численно с помощью градиентного спуска. Градиентный шаг для весов будет выглядеть следующим образом: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_0 \\leftarrow& w_0 - \\frac{2\\eta}{\\ell} \\sum_{i=1}^\\ell{{((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}}, \\\\\n",
    "w_j \\leftarrow& w_j - \\frac{2\\eta}{\\ell} \\sum_{i=1}^\\ell{{x_{ij}((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}},\\ j \\in \\{1,2,3\\}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "Здесь $\\eta$ - параметр, шаг градиентного спуска.\n",
    "\n",
    "Тогда алгоритм на псевдокоде (из ШАДовского учебника по ML): \n",
    "\n",
    "```python\n",
    "w = random_normal()              # можно пробовать и другие виды инициализации\n",
    "repeat S times:                  # другой вариант: while abs(err) > tolerance\n",
    "    f = X.dot(w)                 # посчитать предсказание\n",
    "    err = f - y                  # посчитать ошибку\n",
    "    grad = 2 * X.T.dot(err) / N  # посчитать градиент\n",
    "    w -= alpha * grad            # обновить веса\n",
    "```\n",
    "\n",
    "### Стохастический градиентный спуск\n",
    "\n",
    "Проблема градиентного спуска, описанного выше, в том, что на больших выборках считать на каждом шаге градиент по всем имеющимся данным может быть очень вычислительно сложно. В стохастическом варианте градиентного спуска поправки для весов вычисляются только с учетом одного случайно взятого объекта обучающей выборки: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_0 \\leftarrow& w_0 - \\frac{2\\eta}{\\ell} {((w_0 + w_1x_{k1} + w_2x_{k2} +  w_3x_{k3}) - y_k)}, \\\\ \n",
    "w_j \\leftarrow& w_j - \\frac{2\\eta}{\\ell} {x_{kj}((w_0 + w_1x_{k1} + w_2x_{k2} +  w_3x_{k3}) - y_k)},\\ j \\in \\{1,2,3\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "где $k$ - случайный индекс, $k \\in \\{1, \\ldots, \\ell\\}$.\n",
    "\n",
    "## Литература \n",
    "\n",
    "- [Учебник по ML от ШАД — линейные модели](https://ml-handbook.ru/chapters/linear_models/intro)\n",
    "- [Первый конспект лекции про линейную регрессию из курса ФШЭ](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture02-linregr.pdf)\n",
    "- [Второй конспект лекции про линейную регрессию из курса ФШЭ](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture03-linregr.pdf)\n",
    "- [Теория из курса ODS](https://github.com/Yorko/mlcourse_open/blob/master/jupyter_notebooks/topic04_linear_models/topic4_linear_models_part1_mse_likelihood_bias_variance.ipynb)\n",
    "- [Материалы из курса от МФТИ](https://www.coursera.org/specializations/machine-learning-data-analysis)\n",
    "- [Статья про лин.рег. от ODS на habrahabr](https://habrahabr.ru/company/ods/blog/323890/)\n",
    "\n",
    "В домашнем задании надо будет вычислить всё **вручную** методом нормального уравнения.\n",
    "\n",
    "![](https://ebanoe.it/wp-content/uploads/2016/03/alkogolik-question-how-to-live.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o0xvR9rJUx-"
   },
   "source": [
    "## Практика\n",
    "\n",
    "В этом заданиии мы будем использовать данные [SOCR](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights) по росту и весу 25 тысяч подростков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "rEGAgeA1JUyA",
    "outputId": "0577a8a6-80f5-41a3-f0f9-938489a43cf4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "45oQPpFKJUyI",
    "outputId": "ab2fb6d5-7054-4bb6-ef6b-ed6338e14409"
   },
   "outputs": [],
   "source": [
    "METER_TO_INCH, KILO_TO_POUND = 39.37, 2.20462 # это константы конвертации метров в дюймы, килограмов в фунты\n",
    "\n",
    "df = pd.read_csv('./data/weights_heights.csv.gz', index_col='Index')\n",
    "df['Height'] = df['Height'] / METER_TO_INCH\n",
    "df['Weight'] = df['Weight'] / KILO_TO_POUND\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализируем имеющиеся данные\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "RNAJCaI4JUyP",
    "outputId": "3ec26ead-b1e6-4612-9e1b-b82f025c58f5"
   },
   "outputs": [],
   "source": [
    "# Гистограмма значений роста\n",
    "df.plot(y='Height', kind='hist', \n",
    "        color='red',  title='Height (m.) distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "sddI2E1OJUyU",
    "outputId": "9784de37-fabe-4198-9050-58961f07e4fe"
   },
   "outputs": [],
   "source": [
    "# Гистограмма значений веса\n",
    "df.plot(y='Weight', kind='hist', \n",
    "        color='green',  title='Weight (kg.) distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "-jGIOPmFJUyY",
    "outputId": "a9b51c22-307b-43bd-a190-b9dc1495b714"
   },
   "outputs": [],
   "source": [
    "# посмотрим на scatter плот относительно роста и веса\n",
    "# Видно что есть некоторая линейная зависимость между этими параметрами\n",
    "df.plot(kind='scatter', y='Weight', x='Height', title='Height vs Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно сказать про эти данные по их графикам?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8QlTVQdJUyd"
   },
   "source": [
    "### Среднеквадратичная ошибка\n",
    "\n",
    "В простейшей постановке задача прогноза значения вещественного признака по прочим признакам (задача восстановления регрессии) решается минимизацией квадратичной функции ошибки.\n",
    "\n",
    "В нашем случае у нас 2 признака поэтому будет два параметра $w_0$ и $w_1$\n",
    "\n",
    "$$error(w_0, w_1) = \\sum_{i=1}^n {(y_i - (w_0 + w_1 * x_i))}^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x1, y, w0, w1):\n",
    "    ### Ваш код ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx1SWpRsJUyi"
   },
   "source": [
    "Зафиксируем какие-нибудь значения `w0` и `w1`.  \n",
    "Посмотрим как изменяется значение ошибки `error` в зависимости от `w0`,`w1`.\n",
    "\n",
    "Если всё верно, то в нижней ячейке должны быть выведены числа `209.69670759713378` и `226.27746779896154`, с точностью до 8-10 знака после запятой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "XWlD6A84JUyj",
    "outputId": "9463cacd-b5f9-4598-a7b5-e1b84a29eb62"
   },
   "outputs": [],
   "source": [
    "weight, height = df['Weight'].values, df['Height'].values\n",
    "print(error(weight,height, 1.3, 0.006))\n",
    "print(error(weight,height, 0.6, 0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srKnBoevJUym"
   },
   "source": [
    "Нарисуем на scatter plot эти две прямые, чтобы посмотреть как они \"улавливают\" закономерность.  \n",
    "Очевидно по w0, w1 можно задать уравнение прямой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "1MK38sr2JUyn",
    "outputId": "2d8db417-6d35-40c4-e444-c4486f8365ae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "# alpha - это прозрачность точек, edgecolors - это цвет края точки\n",
    "plt.scatter(x=weight, y=height, edgecolors='black', alpha=0.5)\n",
    "plt.title('Height vs. weight')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Height')\n",
    "x = np.linspace(start=30.0, stop=80.0, num=100)\n",
    "plt.plot(x, 1.3 + 0.006 * x, c='green', linewidth=5)\n",
    "plt.plot(x, 0.6 + 0.02 * x, c='red', linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZdufoDlJUyr"
   },
   "source": [
    "Минимизация квадратичной функции ошибки - относительная простая задача, поскольку функция выпуклая. Для такой задачи существует много методов оптимизации. Посмотрим, как функция ошибки зависит от одного параметра (наклон прямой), если второй параметр (свободный член) зафиксировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "RoywAp6EJUyr",
    "outputId": "9c7c0fe1-0b08-4e8f-c2c4-daf3878fa846"
   },
   "outputs": [],
   "source": [
    "w0 = 1.6 # фиксируем свободный член\n",
    "w1 = np.linspace(start=-0.02, stop=0.02, num=100) # перебираем значения w1\n",
    "\n",
    "err = []\n",
    "for w1_i in w1:\n",
    "    err.append(error(weight, height, w0, w1_i))\n",
    "\n",
    "plt.plot(w1, err)\n",
    "plt.xlabel('w1')\n",
    "plt.ylabel('error')\n",
    "plt.title('error vs. w1 for w0=50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KE2MKxeJUyw"
   },
   "source": [
    "Найдем оптимальное значение для $w_1$ при фиксированном $w_0 = 1.6$. Для этого воспользуемся методом `minimize_scalar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dn9S14y7JUyw",
    "outputId": "97a893c8-ac07-4974-803a-b5a045922d31"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "w0 = 1.6\n",
    "w1_opt = minimize_scalar(\n",
    "    fun=lambda x: error(weight, height, w0, x), \n",
    "    bounds=(-0.2, 0.2)).x\n",
    "w1_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "4M2P00_jJUyz",
    "outputId": "891b7fa3-190f-40ff-b9cb-47c5c7fbacdb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(x=weight, y=height, edgecolors='black', alpha=0.5)\n",
    "plt.title('Height vs. weight')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Height')\n",
    "\n",
    "x = np.linspace(start=30.0, stop=80.0, num=100)\n",
    "plt.plot(x, w0 + w1_opt * x, c='green', linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ysqvTn0JUy5"
   },
   "source": [
    "В прошлых графиках мы фиксировали $w_0$ и рисовали двумерные график ошибок. Но по сути мы должны перебирать и $w_0$ и $w_1$, а это трёхмерный график. Его тоже можно визулизировать. (Если вам нужны интерактивные графики, то посмотрите библиотеку [plotly](https://plotly.com/python))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция работает со всем датасетом и с мешгридом w0/w1\n",
    "# чисто для учебных целей\n",
    "def error(x, y, w0, w1):\n",
    "    return sum(map(lambda xi, yi: ((yi - (w0 + w1 * xi))**2), x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "1M2BDxS6JUy6",
    "outputId": "bb30cd0e-b423-46a1-c45c-7d1199b16ed7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.gca(projection='3d') # get current axis\n",
    "\n",
    "w0 = np.linspace(start=-3, stop=3, num=100)\n",
    "w1 = np.linspace(start=-2, stop=2, num=100)\n",
    "w0, w1 = np.meshgrid(w0, w1)\n",
    "z = error(weight, height, w0, w1)\n",
    "\n",
    "ax.plot_surface(w0, w1, z)\n",
    "ax.set_xlabel('Intercept')\n",
    "ax.set_ylabel('Slope')\n",
    "ax.set_zlabel('Error')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем реализовать то же самое, только с помощью метода наименьших квадратов и для более общего случая.\n",
    "\n",
    "Теория на этот счёт изложена в начале ноутбука.\n",
    "\n",
    "Вспомним самое важное для нас.\n",
    "\n",
    "Нормальное уравнение в матричной форме:\n",
    "\n",
    "$$\\Large X^TXw = X^Ty$$\n",
    "\n",
    "Искомая часть для нас здесь $-$ переменная $w$. Это матрица, т.к. уравнение матричное.\n",
    "\n",
    "Вычислить её можно с помощью функции [numpy.linalg.solve](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.solve.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополните реализацию линейной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresRegression:\n",
    "    # конструктор класса\n",
    "    def __init__(self):\n",
    "        # для ясности заранее объявим поле, по умолчанию оно пустое, но потом будет заполнено вычисленными весами\n",
    "        self.w = None\n",
    "    # магия, которая превращает наш объект в callable\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        return self.predict(x)\n",
    "\n",
    "    # этот метод вычисляет веса модели и записывает их в переменную w у текущего объекта\n",
    "    def fit(self, x, y):\n",
    "        #############\n",
    "        #  Ваш код  #\n",
    "        #############\n",
    "\n",
    "    # этот метод производит вычисление y из уравнения выше (советую сначала вывести на бумаге)\n",
    "    def predict(self, x):\n",
    "        #############\n",
    "        #  Ваш код  #\n",
    "        #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель получилась максимально простой, поэтому для эмуляции свободного члена ($w_0$) нам понадобится модифицировать данные.\n",
    "\n",
    "Раз уж мы решили, что рост откладывается на Oy, то давайте считать, что вес это Х.\n",
    "\n",
    "**Создайте таблицу Х и задайте у неё два столбца - одно это вес из наших исходных данных, а второе это столбец полностью из единичек**\n",
    "\n",
    "Убедитесь, что столбец с весом идёт первым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(columns=['Weight', 'Intercept'])\n",
    "X['Weight'] = weight\n",
    "X['Intercept'] = np.ones(shape=X.shape[0])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучите Вашу самодельную модель и выведите её веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LeastSquaresRegression()\n",
    "\n",
    "clf.fit(X, height)\n",
    "\n",
    "print(clf.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше идёт стадия проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20pAPOlOJUzA"
   },
   "source": [
    "Найдем минимум функции ошибки по трехмерной функции с помощью метода `minimize` и оптимизатора `L-BFGS-B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Q0H9RYT7JUzB",
    "outputId": "5cbaffc9-16f7-4e08-8d80-eed24be3835f"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "(w0_opt, w1_opt) = minimize(\n",
    "    fun=lambda x: error(weight, height, x[0], x[1]), \n",
    "    x0=(0, 0), \n",
    "    bounds=((-3, 3), (-2, 2)), \n",
    "    method='L-BFGS-B').x\n",
    "print(f\"w0_opt={w0_opt}\\nw1_opt={w1_opt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точки в нашем классификаторе и в ячейке сверху должны быть одинаковые. Иначе ошибка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "OprXcw2rJUzG",
    "outputId": "29731f4a-455c-4f3a-ec5d-08b564f70ee9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(x=weight, y=height, edgecolors='black', alpha=0.5)\n",
    "plt.title('Height vs. weight')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Height')\n",
    "\n",
    "x = np.linspace(start=30.0, stop=80.0, num=100)\n",
    "plt.plot(x, w0_opt + w1_opt * x, c='green', linewidth=5)\n",
    "plt.plot(x, clf.w[1] + clf.w[0] * x, c='yellow', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если всё реализовано верно, то зелёная и жёлтая линии должны быть друг на друге"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWShu5MiJUzK"
   },
   "source": [
    "## Регуляризация Lasso и Ridge\n",
    "\n",
    "Рассмотрим, как переобучаются линейные модели, почему так происходит, и выясним, как диагностировать и контролировать переобучение с помощью регуляризации.\n",
    "\n",
    "Возьмем известный нам уже датасет о прокатах велосипедов. Его можно рассматривать в свете регрессии и предсказывать количество прокатов в зависимости от погоды.\n",
    "\n",
    "```\n",
    "season: 1 - весна, 2 - лето, 3 - осень, 4 - зима\n",
    "yr: 0 - 2011, 1 - 2012\n",
    "mnth: от 1 до 12\n",
    "holiday: 0 - нет праздника, 1 - есть праздник\n",
    "weekday: от 0 до 6\n",
    "workingday: 0 - нерабочий день, 1 - рабочий день\n",
    "weathersit: оценка благоприятности погоды от 1 (чистый, ясный день) до 4 (ливень, туман)\n",
    "temp: температура в Цельсиях\n",
    "atemp: температура по ощущениям в Цельсиях\n",
    "hum: влажность\n",
    "windspeed(mph): скорость ветра в милях в час\n",
    "windspeed(ms): скорость ветра в метрах в секунду\n",
    "cnt: количество арендованных велосипедов (это целевой признак, его мы будем предсказывать)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "V9szS2kuJUzL",
    "outputId": "7cc04673-d772-4b36-b923-5f9a2f000ba9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bikes_rent.csv.gz') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "5ETN83hBJUzN",
    "outputId": "ca63276f-b973-4a51-bbb5-36b0ba900f27"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqe_uqX-JUzQ"
   },
   "source": [
    "Построим матрицу корреляций признаков, чтобы найти линейно зависимые признаки.  \n",
    "Обратите внимание на то, что мы берем абсолютное значение корреляции. Нам не важно как коррелируют - положительно или отрицательно. Главное, что чем больше корреляция или антикорреляция, тем сильнее линейная связь между признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# корреляционная матрица\n",
    "corr = np.abs(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# эта маска нужна для того, чтобы красиво вывести heatmap, без дублирования данных\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "t_8k-nA9JUzQ",
    "outputId": "8bd5c212-56d4-44cd-fe7c-e1d6a6f10070"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr,cmap=cmap, square=True,linewidths=.5, mask=mask,annot=True, fmt=\".1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85X1Dg2yJUzU"
   },
   "source": [
    "Судя по корреляционной матрице есть несколько признаков который сильно коррелируют с целевой переменной - числом прокатов (`cnt`):\n",
    "\n",
    "- yr\n",
    "- temp\n",
    "- atemp\n",
    "- season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXtVdF4KJUzU"
   },
   "source": [
    "Как мы помними в линейной регрессии используются взвешенные суммы признаков, поэтому все они должны быть одного масштаба. (Что бы избежать расхождения алгоритма градиентного спуска) Проверим, так ли это у нас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "eTnFaV44JUzU",
    "outputId": "9abd4149-9d85-4495-8ce9-73f35075ff96"
   },
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AlOqy5HJUzZ"
   },
   "source": [
    "Как мы видим все они разного масштаба, следовательно их требуется отмасштабировать. Можно использовать простой `sklearn.preprocessing.StandardScaler`. Суть его очень простая - приводить всё к нулевому среднему и масштабировать до единичного значения разброса. Получается такая формула для вычисления значения признака: \n",
    "$$x_i = \\frac{x_i - mean(X)}{stdev(X)}$$\n",
    "\n",
    "Тогда $Mean(X) = 0$ и $Stdev(X) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x58-SBxJJUzZ",
    "outputId": "b9da6b78-ea1e-43db-f9f1-63dd76bdc3b0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Посмотим еще раз на изначальные распределения признаков\n",
    "\n",
    "sns.kdeplot(df['temp'])\n",
    "sns.kdeplot(df['atemp'])\n",
    "sns.kdeplot(df['windspeed(ms)'])\n",
    "sns.kdeplot(df['windspeed(mph)'])\n",
    "sns.kdeplot(df['hum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw0ssZusJUzb"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf2XiJNiJUze",
    "outputId": "0d3479ad-d5bc-4699-ca1f-dc784ad1ccc7"
   },
   "outputs": [],
   "source": [
    "df = shuffle(df, random_state=42)\n",
    "X = df.drop(['cnt'], axis=1)\n",
    "y = df['cnt']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2lyOBjkJUzi",
    "outputId": "79ec5ce4-4c40-437f-f94f-4e375a5cbf8e"
   },
   "outputs": [],
   "source": [
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOCr1cMHJUzl",
    "outputId": "f8f1d8c6-6b1b-4002-cf0b-0480a7bdfa0c"
   },
   "outputs": [],
   "source": [
    "X.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ7uQEOhJUzn"
   },
   "source": [
    "Как видим стандартное отклонение теперь единично, а среднее близко к нулю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClLgIhFqJUzq",
    "outputId": "2c2e2573-46c5-4ca1-b6f4-7e6061119496"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for i in range(X.shape[1]):\n",
    "    sns.kdeplot(X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zio8pxMZJUzt"
   },
   "source": [
    "Теперь обучим линейную модель. В этот раз мы будем делать это не вручную, а использовать класс `LinearRegression`.\n",
    "\n",
    "Воспользуемся им для начала без параметров, то есть со значениями по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgwjQIrTJUzt",
    "outputId": "b52dbe70-4db2-4fb5-dbb7-073b93e09b91"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)  # обучение. Может выдать warning про LAPACK - всё ок\n",
    "for coef, col in sorted(zip(lr.coef_, df.columns), key=lambda x: np.abs(x[0])):\n",
    "    print(\"{} \\t {}\".format(np.round(coef, 4), col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJW_0lUcJUzx"
   },
   "source": [
    "Посмотрим на магнитуду признаков, то есть на зхначения весов в линейной модели при каждом признаке. Мы помним, что чем , больше по модулю коэффициент, тем больше \"веса\" он имеет в модели и тем важнее признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBWXCPoaJUzy",
    "outputId": "e4f9bf52-9301-4694-a0d0-9c99aac3fcbd"
   },
   "outputs": [],
   "source": [
    "# Магнитуда признаков\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.barplot(y='columns', x='coef',data=pd.DataFrame(list(zip(lr.coef_, df.columns)), columns=[\"coef\", \"columns\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnXP497XJUz2"
   },
   "source": [
    "Мы видим, что веса при линейно-зависимых признаках по модулю значительно больше, чем при других признаках.\n",
    "Чтобы понять, почему так произошло, вспомним аналитическую формулу, по которой вычисляются веса линейной модели в методе наименьших квадратов: $w = (X^TX)^{-1} X^T y$.\n",
    "\n",
    "Если в $X$ есть коллинеарные (линейно-зависимые) столбцы, матрица $X^TX$ становится вырожденной, и формула перестает быть корректной. Чем более зависимы признаки, тем меньше определитель этой матрицы и тем хуже аппроксимация $Xw \\approx y$. Такая ситуацию называют проблемой мультиколлинеарности, мы обсуждали ее на лекции.\n",
    "\n",
    "С парой `temp`-`atemp` чуть менее коррелирующих переменных такого не произошло, однако на практике всегда стоит внимательно следить за коэффициентами при похожих признаках.\n",
    "\n",
    "Решение проблемы мультиколлинеарности состоит в регуляризации линейной модели. К оптимизируемому функционалу прибавляют $L_1$ или $L_2$ норму весов, умноженную на коэффициент регуляризации $\\alpha$. В первом случае метод называется **Lasso**, а во втором – **Ridge**.\n",
    "\n",
    "#### Lasso (L1)\n",
    "\n",
    "$$\\Large error(X, y, w) = \\frac{1}{2} \\sum_{i=1}^\\ell {(y_i - w^Tx_i)}^2 + \\alpha \\sum_{i=1}^d |w_i|$$, где $\\alpha$ - это коэффициент регуляризации.\n",
    "\n",
    "#### Ridge (L2)\n",
    "\n",
    "$$\\Large error(X, y, w) = \\frac{1}{2} \\sum_{i=1}^\\ell {(y_i - w^Tx_i)}^2 + \\alpha \\sum_{i=1}^d w_i^2$$, где $\\alpha$ - это коэффициент регуляризации.\n",
    "\n",
    "Для Ridge (Так же известного как регуляризация Тихонова) можно переформулировать матричное уравнение в нормальной форме:\n",
    "$$\\Large (X^TX - \\lambda I)w = X^Ty$$\n",
    "А для Lasso нужен координатный спуск. Про концепцию алгоритма вы можете почитать [здесь](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BF%D0%BE%D0%BA%D0%BE%D0%BE%D1%80%D0%B4%D0%B8%D0%BD%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0).\n",
    "\n",
    "---\n",
    "\n",
    "Рассмотрим на примере как разная регуляризация влияет на веса $w$ модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем реализовать Ridge руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ridge(LeastSquaresRegression):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        super(Ridge, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        #############\n",
    "        #  Ваш код  #\n",
    "        #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X, y)\n",
    "\n",
    "for coef, col in sorted(zip(ridge.w, df.columns), key=lambda x: np.abs(x[0])):\n",
    "    print(\"{} \\t {}\".format(np.round(coef, 4), col))\n",
    "    \n",
    "# Магнитуда признаков\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.barplot(y='columns', x='coef',data=pd.DataFrame(list(zip(ridge.w, df.columns)), columns=[\"coef\", \"columns\"]))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сравните наш Ridge с библиотечным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYOsC_H_JUz3",
    "outputId": "94827b5e-1ae1-433d-f0ef-7cb3f1031707"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "ridge = Ridge()\n",
    "ridge.fit(X, y)\n",
    "\n",
    "for coef, col in sorted(zip(ridge.coef_, df.columns), key=lambda x: np.abs(x[0])):\n",
    "    print(\"{} \\t {}\".format(np.round(coef, 4), col))\n",
    "    \n",
    "# Магнитуда признаков\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.barplot(y='columns', x='coef',data=pd.DataFrame(list(zip(ridge.coef_, df.columns)), columns=[\"coef\", \"columns\"]))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCAjs4xoJUz6",
    "outputId": "8ee56ac4-fcb5-4461-9c2c-a5f7099f0b7e"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X, y)\n",
    "\n",
    "for coef, col in sorted(zip(lasso.coef_, df.columns), key=lambda x: np.abs(x[0])):\n",
    "    print(\"{} \\t {}\".format(np.round(coef, 4), col))\n",
    "    \n",
    "plt.figure(figsize=(14,8))\n",
    "sns.barplot(y='columns', x='coef',data=pd.DataFrame(list(zip(lasso.coef_, df.columns)), columns=[\"coef\", \"columns\"]))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0rL4NyyJUz9"
   },
   "source": [
    "Как мы видим, Lasso контролирует веса модели **обнуляя веса** при бесполезных признаках.  \n",
    "Ridge просто **контролирует величину** коэффициентов. \n",
    "\n",
    "\n",
    "В любом случае регуляризатор просто вводит штрафы на очень большие веса $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD-Qom0SJUz9"
   },
   "source": [
    "#### Коэффициент регуляризации\n",
    "\n",
    "Посмотрим как изменение коэффициента регуляризации влияет на веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HsHI0WsJUz-"
   },
   "outputs": [],
   "source": [
    "alphas = np.arange(1, 500, 5)\n",
    "coefs_lasso = np.zeros((alphas.shape[0], X.shape[1])) # матрица весов размера (число регрессоров) x (число признаков)\n",
    "coefs_ridge = np.zeros((alphas.shape[0], X.shape[1]))\n",
    "\n",
    "i = 0\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha = alpha, random_state=42)\n",
    "    lasso.fit(X, y)\n",
    "    coefs_lasso[i, :] = lasso.coef_\n",
    "    \n",
    "    ridge = Ridge(alpha = alpha, random_state=42)\n",
    "    ridge.fit(X, y)\n",
    "    coefs_ridge[i, :] = ridge.coef_\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdSCFQWwJU0A",
    "outputId": "0a25f16a-3858-4f01-83c0-6e24a01bc115"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for coef, feature in zip(coefs_lasso.T, df.columns):\n",
    "    plt.plot(alphas, coef, label=feature, color=np.random.rand(3), linewidth=3)\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.4, 0.95))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"feature weight\")\n",
    "plt.title(\"Lasso\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for coef, feature in zip(coefs_ridge.T, df.columns):\n",
    "    plt.plot(alphas, coef, label=feature, color=np.random.rand(3), linewidth=3)\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.4, 0.95))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"feature weight\")\n",
    "plt.title(\"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuTeMZlIJU0C"
   },
   "source": [
    "- Судя по графику Lasso($L_1$) агрессивнее уменьшает веса, чем Ridge($L_2$)\n",
    "- В случае с $L_1$ регуляризатором, увеличение alpha сколлапсирует все веса в ноль. Это происходит из-за характера функции регуляризации. В $L_1$ - это $sum(|W|) * alpha$ в то время как в $L_2$ это $sum(W^2) * alpha$. Тогда $L_1$ слагамемое регуляризации будет больше чем слагаемое $L_2$ регуляризации. А оптимальное значение будет достигаться в 0. Кроме того $L_1$ производит отбор признаков, зануляя неинформативные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xaoppI5JU0D"
   },
   "source": [
    "### Выбор лучшего alpha\n",
    "\n",
    "Итак, мы видим, что при изменении alpha модель по-разному подбирает коэффициенты признаков. Нам нужно выбрать наилучшее alpha.\n",
    "\n",
    "Для этого, во-первых, нам нужна метрика качества. Будем использовать в качестве метрики сам оптимизируемый функционал метода наименьших квадратов, то есть _Mean Square Error_.\n",
    "\n",
    "Во-вторых, нужно понять, на каких данных эту метрику считать. Нельзя выбирать alpha по значению MSE на обучающей выборке, потому что тогда мы не сможем оценить, как модель будет делать предсказания на новых для нее данных. Если мы выберем одно разбиение выборки на обучающую и тестовую (это называется holdout), то настроимся на конкретные \"новые\" данные, и вновь можем переобучиться. Поэтому будем делать несколько разбиений выборки, на каждом пробовать разные значения alpha, а затем усреднять MSE. Удобнее всего делать такие разбиения кросс-валидацией, то есть разделить выборку на K частей, или блоков, и каждый раз брать одну из них как тестовую, а из оставшихся блоков составлять обучающую выборку.\n",
    "\n",
    "Делать кросс-валидацию для регрессии в sklearn совсем просто: для этого есть специальный регрессор, LassoCV, который берет на вход список из alpha и для каждого из них вычисляет MSE на кросс-валидации. После обучения (если оставить параметр `cv=3`) регрессор будет содержать переменную `mse_path_`, матрицу размера `len(alpha)` x `k`, `k = 3` (число блоков в кросс-валидации), содержащую значения MSE на тесте для соответствующих запусков. Кроме того, в переменной `alpha_` будет храниться выбранное значение параметра регуляризации, а в `coef_`, традиционно, обученные веса, соответствующие этому `alpha_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22sD2ULNJU0M",
    "outputId": "7529fc41-cf81-4e2d-d234-f3ca594bba08"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas = np.linspace(0.001, 50, 500)\n",
    "lasso_cv = LassoCV(alphas=alphas, random_state=42)\n",
    "lasso_cv.fit(X, y)\n",
    "\n",
    "mean_mse = np.mean(lasso_cv.mse_path_, axis = 1)\n",
    "\n",
    "plt.plot(lasso_cv.alphas_, mean_mse)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Lasso')\n",
    "print('alpha = {}'.format(lasso_cv.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_9KMLAKJU0O"
   },
   "source": [
    "Посмотрим на значения коэффициентов при оптимальном alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhPH-1ssJU0P",
    "outputId": "811e0d03-47a4-44a2-cb15-5778aa72107a"
   },
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(list(zip(np.round(lasso_cv.coef_, 2), df.columns)))\n",
    "coef.columns = ['weight', 'feature']\n",
    "coef.sort_values(['weight'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmTWVDWjJU0S"
   },
   "source": [
    "После построения модели, подбора параметров и обучения наступает этап анализа результатов.  \n",
    "На сколько хорошо модель строит предсказания, можно ли ей пользоваться, где она ошибается чаще всего? \n",
    "\n",
    "Для того, чтобы ответить на эти вопросы, мы сделаем следующее:\n",
    "\n",
    "1. Проверим с помощью кросс валидации значение средней ошибки. Кросс валидация - это случайное разбиение выборки на части, обучение модели и вычисление средней точности модели.\n",
    "2. Оценим долю ошибок\n",
    "3. Визуализируем ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsxh-NemJU0S"
   },
   "source": [
    "### Кросс валидация\n",
    "\n",
    "> Этот блок взят из курса ODS: https://habr.com/company/ods/blog/322534/\n",
    "\n",
    "Главная задача обучаемых алгоритмов – их способность обобщаться, то есть хорошо работать на новых данных. Поскольку на новых данных мы сразу не можем проверить качество построенной модели (нам ведь надо для них сделать прогноз, то есть истинных значений целевого признака мы для них не знаем), то надо пожертвовать небольшой порцией данных, чтоб на ней проверить качество модели.\n",
    "\n",
    "\n",
    "Чаще всего это делается одним из 2 способов:\n",
    "\n",
    "- отложенная выборка (held-out/hold-out set). При таком подходе мы оставляем какую-то долю обучающей выборки (как правило от 20% до 40%), обучаем модель на остальных данных (60-80% исходной выборки) и считаем некоторую метрику качества модели (например, самое простое – долю правильных ответов в задаче классификации) на отложенной выборке.\n",
    "- кросс-валидация (cross-validation, на русский еще переводят как скользящий или перекрестный контроль). Тут самый частый случай – K-fold кросс-валидация.\n",
    "\n",
    "![image.png](https://habrastorage.org/files/b1d/706/e6c/b1d706e6c9df49c297b6152878a2d03f.png)\n",
    "\n",
    "Тут модель обучается  раз на разных () подвыборках исходной выборки (белый цвет), а проверяется на одной подвыборке (каждый раз на разной, оранжевый цвет).\n",
    "Получаются  оценок качества модели, которые обычно усредняются, выдавая среднюю оценку качества классификации/регрессии на кросс-валидации.\n",
    "\n",
    "\n",
    "Кросс-валидация дает лучшую по сравнению с отложенной выборкой оценку качества модели на новых данных. Но кросс-валидация вычислительно дорогостоящая, если данных много.\n",
    "\n",
    "\n",
    "Кросс-валидация – очень важная техника в машинном обучении (применяемая также в статистике и эконометрике), с ее помощью выбираются гиперпараметры моделей, сравниваются модели между собой, оценивается полезность новых признаков в задаче и т.д. Более подробно можно почитать, например, [тут у Sebastian Raschka](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html) или в любом классическом учебнике по машинному (статистическому) обучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN7iBVQJJU0S",
    "outputId": "0fece3a7-52db-4edc-b873-20dfa289995e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv = 3 # проведем 3 эксперимента\n",
    "errors = cross_val_score(Lasso(lasso_cv.alpha_), X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "print(errors)\n",
    "error = abs(np.mean(errors))\n",
    "print(\"error={:0.3f}, std={:0.4f}\".format(error, np.std(errors)))\n",
    "\n",
    "# судя по отклонению модель получилась не очень стабильной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMQXoiLAJU0V"
   },
   "source": [
    "Но что вообще означает эта ошибка?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toAOjS8KJU0V",
    "outputId": "3cffeba9-6b76-44fa-d446-a63f4478b1a4"
   },
   "outputs": [],
   "source": [
    "total_cnt = df.cnt.sum()\n",
    "print(\"Прокатов было совершено за всё время\", total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUICPc3wJU0W",
    "outputId": "3bc9e8d4-0f61-4da7-dc14-078f0f0a61ee"
   },
   "outputs": [],
   "source": [
    "\"Доля ошибок: {:.2%}\".format((error*X.shape[0]) / total_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWTSFNmuJU0Z"
   },
   "source": [
    "### Визуализация ошибки регрессии\n",
    "\n",
    "Интересно же посмотреть на графике где и как ошибается предсказание!\n",
    "\n",
    "Для начала разобьем всю выборку на test и train. То есть на train сегменте мы будем обучать модель, а на test - проверять качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGsBMA1GJU0Z",
    "outputId": "b92f2cb8-5c6d-4daa-ae2e-9cdb99a2b80a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(\"train_shape={}, test_shape={}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "reg = Lasso(lasso_cv.alpha_)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFI1bYRIJU0b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Построим таблицу предсказание / оригинальное значение / MSE / MAE\n",
    "prediction = pd.DataFrame(np.vstack((pred, y_test)).T, columns=[\"pred\", \"target\"])\n",
    "prediction['MSE'] = np.power(prediction['pred'] - prediction['target'], 2)\n",
    "prediction['MAE'] = np.abs(prediction['pred'] - prediction['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvWljspTJU0d",
    "outputId": "c68a79f3-2a0e-445d-8c40-0e45983d2775"
   },
   "outputs": [],
   "source": [
    "# Сортируем по убыванию ошибки\n",
    "prediction.sort_values(by='MAE', ascending=False, inplace=True)\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_3m7BsiJU0f",
    "outputId": "4096d2fc-2a88-401d-b677-2329dcd0be04"
   },
   "outputs": [],
   "source": [
    "# Сортируем по возрастанию ошибки\n",
    "prediction.sort_values(by='MAE', inplace=True)\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDCzD7z_JU0g",
    "outputId": "2bf5d170-29d8-4d36-a930-4c26af66974e"
   },
   "outputs": [],
   "source": [
    "print(\"общая ошибка предсказателя в количестве прокатов = {:0.1f}\\nвсего прокатов = {:0.1f}\\nдоля ошибок = {:.2%}\"\n",
    "      .format(prediction.MAE.sum(), prediction.target.sum(), prediction.MAE.sum() / prediction.target.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m21XGHAQJU0k",
    "outputId": "dc934bcc-8818-4332-d2c2-06483859bff4"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0, prediction.shape[0]), prediction['MAE'])\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH6BndvNJU0l"
   },
   "source": [
    "Финальная визуализация:  \n",
    "На одном графике target и prediction. Точки отсортированы по величине ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzuiEustJU0l",
    "outputId": "f2068fa6-967a-42e7-afe5-0178dd165cb8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "plt.plot(range(0, prediction.shape[0]), prediction['pred'])\n",
    "plt.plot(range(0, prediction.shape[0]), prediction['target'])\n",
    "plt.plot(range(0, prediction.shape[0]), prediction['MAE'])\n",
    "plt.legend([\"Predicted\", \"Target\", \"MAE\"])\n",
    "\n",
    "plt.xlabel(\"Element index\")\n",
    "plt.ylabel(\"Target/Predicted value\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O4GXW5hJU0n"
   },
   "source": [
    "Вывод делайте сами, достаточно ли погрешности в 15% от всего проката или нет :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfK7iHwfJU0n"
   },
   "source": [
    "## Бонус секция\n",
    "\n",
    "Пример с полиномиальной регрессией и проблемой мультиколлинеарности, как было рассказано на лекции.\n",
    "\n",
    "Взято из материалов мастеркласса Aiarlabs https://github.com/mephistopheies/mlworkshop39_042017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoLH0VuhJU0n"
   },
   "outputs": [],
   "source": [
    "def generate_wave_set(n_support=1000, n_train=25, std=0.3):\n",
    "    data = {}\n",
    "    # create 1000 points uniformly distibuted in closed intervl from 0 to 2*pi\n",
    "    # it is some kind of resolution for sampling\n",
    "    data['support'] = np.linspace(0, 2*np.pi, num=n_support)\n",
    "    # calculate sine values\n",
    "    data['values'] = np.sin(data['support']) + 1\n",
    "    # choose n_train random vakues from support (with replacement) and sort them ascending\n",
    "    data['x_train'] = np.sort(np.random.choice(data['support'], size=n_train, replace=True))\n",
    "    # calculate sine for each of samplesd point\n",
    "    data['y_train'] = np.sin(data['x_train']) + 1 + np.random.normal(0, std, size=data['x_train'].shape[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l795xHNHJU0q",
    "outputId": "00955791-456d-4d9f-e6f6-c674f3254399"
   },
   "outputs": [],
   "source": [
    "# sample 25 points from 1000 available from noised sin manifold \n",
    "data = generate_wave_set(1000, 25)\n",
    "\n",
    "print('Shape of X is', data['x_train'].shape)\n",
    "print('Head of X is', data['x_train'][:10])\n",
    "\n",
    "margin = 0.3\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "plt.xlim(data['x_train'].min() - margin, data['x_train'].max() + margin)\n",
    "plt.ylim(data['y_train'].min() - margin, data['y_train'].max() + margin)\n",
    "plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.title('True manifold and noised data', fontsize=20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llgnLUaWJU0t",
    "outputId": "15abcd66-f4c4-47b1-9560-68be52f41475"
   },
   "outputs": [],
   "source": [
    "# define list with degrees of polynomials to investigate\n",
    "degree_list = [1, 2, 3, 10, 12, 13]\n",
    "# get color palette\n",
    "cmap = plt.get_cmap('jet')\n",
    "# compute individual color for each curve\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(degree_list))]\n",
    "\n",
    "margin = 0.3\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "\n",
    "# save weights of all fitted polynomial regressions\n",
    "w_list = []\n",
    "err = []\n",
    "for ix, degree in enumerate(degree_list):\n",
    "    # list with polynomial features for each degree\n",
    "    dlist = [np.ones(data['x_train'].shape[0])] + list(map(lambda n: data['x_train']**n, range(1, degree + 1)))\n",
    "    X = np.array(dlist).T\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), data['y_train'])\n",
    "    w_list.append((degree, w))\n",
    "    y_hat = np.dot(w, X.T)\n",
    "    err.append(np.mean((data['y_train'] - y_hat)**2))\n",
    "    plt.plot(data['x_train'], y_hat, color=colors[ix], label='poly degree: %i' % degree)\n",
    "\n",
    "plt.xlim(data['x_train'].min() - margin, data['x_train'].max() + margin)\n",
    "plt.ylim(data['y_train'].min() - margin, data['y_train'].max() + margin)\n",
    "plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.title('Fitted polynomial regressions', fontsize=20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск в лин регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.array([[x] for x in np.arange(0.0, 6.0, 0.5)])\n",
    "y = np.array([[2*x[0] + 10] for x in x.tolist()])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас у нас строго *линейная зависимость*. Добавим немного шума, используя метод `np.random.rand`\n",
    "\n",
    "Воспользуемся `np.random.seed(someNumber)`, чтобы зафиксировать случайность и получать при перезапуске ячейки\n",
    "стабильный результат.\n",
    "\n",
    "**Hint**\n",
    "- [Доки по np.random](https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.random.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.array([[x] for x in np.arange(0.0, 6.0, 0.5)])\n",
    "y = np.array([[2*x[0] + 10 + np.random.rand()*3] for x in x.tolist()])\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявим параметры: веса и шаг градиентного спуска\n",
    "\n",
    "w0 = np.random.rand()\n",
    "w1 = np.random.rand()\n",
    "step = 0.008\n",
    "\n",
    "# Как видно, случайно инициализированные веса не очень хорошо приблизили линейную зависимость.\n",
    "# Точнее вообще никак не приблизили\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, x*w1 + w0, c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем само обучение\n",
    "# Число шагов для обучения установим в 30\n",
    "\n",
    "for i in range(30):\n",
    "    \n",
    "    # Предсказание\n",
    "    pred = x*w1 + w0\n",
    "    \n",
    "    # MSE-loss\n",
    "    loss = np.sum((y - pred)**2)/x.shape[0]\n",
    "    \n",
    "    # шаг для каждого из весов\n",
    "    w0 = w0 - step*np.sum((pred - y) * x) * 2 / x.shape[0]\n",
    "    w1 = w1 - step*np.sum(pred - y) * 2 / x.shape[0]\n",
    "    \n",
    "    # Критерий остановки обучения:\n",
    "    # разница между значениями MSE меньше, чем на 0.01\n",
    "    # Вычтем из старого значения лосс-функции новое, полученное уже после обновления весов. \n",
    "    if np.abs(loss - np.sum((x*w1 + w0 - y)**2) / x.shape[0]) < 0.01:\n",
    "        print(loss)\n",
    "        break\n",
    "        \n",
    "\n",
    "# Теперь красная линия приближает данные лучше.        \n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, x*w1 + w0, c='red')\n",
    "\n",
    "w1, w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание \n",
    "\n",
    "Реализовать модели линейной регрессии в виде класса, способного работать как с одномерным набором признаков(как в примере выше), так и с многомерным (когда у нас есть x_1, x_2 и тд).  \n",
    "Веса желательно задать в виде вектора, X преобразовать, добавив в начало единицу (см. теорию).  \n",
    "Используйте возможности библиотеки numpy (например, при использовании матриц, пользуйтесь [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [np.transpose](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)).\n",
    "\n",
    "**Учтите**, что признаки - это `numpy`-массивы:  таблицы с данными легко превратить в numpy-массив (это вы увидите позже), а сами по себе такие массивы удобнее, чем стандартные питоновские списки, в первую очередь за счет того, что \n",
    "- numpy быстрее, чем питон, так как под капотом написан на C,\n",
    "- в библиотеке реализовано множество полезных методов, в чем вы уже могли убедиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGrad:\n",
    "    def __init__(self, lr = 0.0001, iters = 5000) -> None:\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        # TODO: напиши меня!\n",
    "    \n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO: напиши меня!\n",
    "    \n",
    "    def _getBiased(self, x):\n",
    "        # это подсказка, помните мы добавляли фейковую колонку с 1 в датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее проверки работы вашего алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "lr = LinearRegressionGrad(iters=3000, lr=0.001)\n",
    "lr.fit(x, y.reshape(-1))\n",
    "pred = lr.predict(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, pred, c='red')\n",
    "\n",
    "lr.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим размерности\n",
    "assert pred.shape == np.array(y.reshape(12)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на реальном датасете\n",
    "\n",
    "Загрузим датасет с [прокатами велосипедов](https://www.kaggle.com/c/bike-sharing-demand/data).\n",
    "\n",
    "Попробуем решить задачу регрессии, и предсказать число прокатов в зависимости от погоды.  \n",
    "Описание всех признаков:  \n",
    "\n",
    "```\n",
    "season: 1 - весна, 2 - лето, 3 - осень, 4 - зима\n",
    "yr: 0 - 2011, 1 - 2012\n",
    "mnth: от 1 до 12\n",
    "holiday: 0 - нет праздника, 1 - есть праздник\n",
    "weekday: от 0 до 6\n",
    "workingday: 0 - нерабочий день, 1 - рабочий день\n",
    "weathersit: оценка благоприятности погоды от 1 (чистый, ясный день) до 4 (ливень, туман)\n",
    "temp: температура в Цельсиях\n",
    "atemp: температура по ощущениям в Цельсиях\n",
    "hum: влажность\n",
    "windspeed(mph): скорость ветра в милях в час\n",
    "windspeed(ms): скорость ветра в метрах в секунду\n",
    "cnt: количество арендованных велосипедов (это целевой признак, его мы будем предсказывать)\n",
    "```\n",
    "\n",
    "Предсказывать мы будем `cnt`, все остальные значения - нецелевые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bikes_rent.csv.gz') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(df['temp'])\n",
    "sns.kdeplot(df['atemp'])\n",
    "sns.kdeplot(df['windspeed(ms)'])\n",
    "sns.kdeplot(df['windspeed(mph)'])\n",
    "sns.kdeplot(df['hum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScaler(x):\n",
    "    mean = x - np.mean(x)\n",
    "    return mean / np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разобьем выборку на целевые и нецелевые признаки\n",
    "# удалим windspeed(ms) сразу, так как мы уже знаем, что это плохой признак\n",
    "x = df.drop(['cnt', 'windspeed(ms)'], axis=1)\n",
    "y = df['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = standardScaler(x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for i in x_scaled.columns:\n",
    "    sns.kdeplot(x_scaled[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее должно быть околонулевым значением, а отклонение равно единице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(x_scaled.values))\n",
    "print(np.std(x_scaled.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionGrad(iters=5000, lr=0.001)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "mae = np.abs(pred - y_test).mean()\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "pred = ridge.predict(X_test)\n",
    "\n",
    "mae = np.abs(pred - y_test).mean()\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_, ridge.intercept_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tsxh-NemJU0S",
    "fWTSFNmuJU0Z"
   ],
   "name": "02-ws-linear-regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
